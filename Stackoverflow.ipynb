{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import datetime as dt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score,precision_score,recall_score\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from skmultilearn.adapt import mlknn\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "if not os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    disk_engine = create_engine('sqlite:///train.db')\n",
    "    start = dt.datetime.now()\n",
    "    chunksize = 180000\n",
    "    j = 0\n",
    "    index_start = 1\n",
    "    for df in pd.read_csv('Train/Train.csv', names=['Id', 'Title', 'Body', 'Tags'], chunksize=chunksize, iterator=True, encoding='utf-8', ):\n",
    "        df.index += index_start\n",
    "        j+=1\n",
    "        print('{} rows'.format(j*chunksize))\n",
    "        df.to_sql('data', disk_engine, if_exists='append')\n",
    "        index_start = df.index[-1] + 1\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the database : \n",
      " 6034196\n",
      "Time taken to count the number of rows : 0:01:02.618873\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    num_rows = pd.read_sql_query(\"\"\"SELECT count(*) FROM data\"\"\", con)\n",
    "    #Always remember to close the database\n",
    "    print(\"Number of rows in the database :\",\"\\n\",num_rows['count(*)'].values[0])\n",
    "    con.close()\n",
    "    print(\"Time taken to count the number of rows :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cell to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 3:24:47.490392\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('train.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train.db')\n",
    "    df_no_dup = pd.read_sql_query('SELECT Title, Body, Tags, COUNT(*) as cnt_dup FROM data GROUP BY Title, Body, Tags', con)\n",
    "    con.close()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the first to genarate train.db file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>cnt_dup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Implementing Boundary Value Analysis of S...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;\\n#include&amp;...</td>\n",
       "      <td>c++ c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding columns</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.lang.NoClassDefFoundError: javax/serv...</td>\n",
       "      <td>&lt;p&gt;I followed the guide in &lt;a href=\"http://sta...</td>\n",
       "      <td>jsp jstl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java.sql.SQLException:[Microsoft][ODBC Dri...</td>\n",
       "      <td>&lt;p&gt;I use the following code&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;...</td>\n",
       "      <td>java jdbc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0       Implementing Boundary Value Analysis of S...   \n",
       "1           Dynamic Datagrid Binding in Silverlight?   \n",
       "2           Dynamic Datagrid Binding in Silverlight?   \n",
       "3      java.lang.NoClassDefFoundError: javax/serv...   \n",
       "4      java.sql.SQLException:[Microsoft][ODBC Dri...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <pre><code>#include&lt;iostream&gt;\\n#include&...   \n",
       "1  <p>I should do binding for datagrid dynamicall...   \n",
       "2  <p>I should do binding for datagrid dynamicall...   \n",
       "3  <p>I followed the guide in <a href=\"http://sta...   \n",
       "4  <p>I use the following code</p>\\n\\n<pre><code>...   \n",
       "\n",
       "                                  Tags  cnt_dup  \n",
       "0                                c++ c        1  \n",
       "1          c# silverlight data-binding        1  \n",
       "2  c# silverlight data-binding columns        1  \n",
       "3                             jsp jstl        1  \n",
       "4                            java jdbc        2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate questions : 1827881 ( 30.292038906260256 % )\n"
     ]
    }
   ],
   "source": [
    "print(\"number of duplicate questions :\", num_rows['count(*)'].values[0]- df_no_dup.shape[0], \"(\",(1-((df_no_dup.shape[0])/(num_rows['count(*)'].values[0])))*100,\"% )\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2656284\n",
       "2    1272336\n",
       "3     277575\n",
       "4         90\n",
       "5         25\n",
       "6          5\n",
       "Name: cnt_dup, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.cnt_dup.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dup.dropna(how='any',axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run this cell : 0:00:09.246361\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>cnt_dup</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Implementing Boundary Value Analysis of S...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;\\n#include&amp;...</td>\n",
       "      <td>c++ c</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding columns</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.lang.NoClassDefFoundError: javax/serv...</td>\n",
       "      <td>&lt;p&gt;I followed the guide in &lt;a href=\"http://sta...</td>\n",
       "      <td>jsp jstl</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java.sql.SQLException:[Microsoft][ODBC Dri...</td>\n",
       "      <td>&lt;p&gt;I use the following code&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;...</td>\n",
       "      <td>java jdbc</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0       Implementing Boundary Value Analysis of S...   \n",
       "1           Dynamic Datagrid Binding in Silverlight?   \n",
       "2           Dynamic Datagrid Binding in Silverlight?   \n",
       "3      java.lang.NoClassDefFoundError: javax/serv...   \n",
       "4      java.sql.SQLException:[Microsoft][ODBC Dri...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <pre><code>#include&lt;iostream&gt;\\n#include&...   \n",
       "1  <p>I should do binding for datagrid dynamicall...   \n",
       "2  <p>I should do binding for datagrid dynamicall...   \n",
       "3  <p>I followed the guide in <a href=\"http://sta...   \n",
       "4  <p>I use the following code</p>\\n\\n<pre><code>...   \n",
       "\n",
       "                                  Tags  cnt_dup  tag_count  \n",
       "0                                c++ c        1          2  \n",
       "1          c# silverlight data-binding        1          3  \n",
       "2  c# silverlight data-binding columns        1          4  \n",
       "3                             jsp jstl        1          2  \n",
       "4                            java jdbc        2          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "df_no_dup[\"tag_count\"] = df_no_dup[\"Tags\"].apply(lambda text: len(text.split(\" \")))\n",
    "# adding a new feature number of tags per question\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "      <th>Tags</th>\n",
       "      <th>cnt_dup</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Implementing Boundary Value Analysis of S...</td>\n",
       "      <td>&lt;pre&gt;&lt;code&gt;#include&amp;lt;iostream&amp;gt;\\n#include&amp;...</td>\n",
       "      <td>c++ c</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dynamic Datagrid Binding in Silverlight?</td>\n",
       "      <td>&lt;p&gt;I should do binding for datagrid dynamicall...</td>\n",
       "      <td>c# silverlight data-binding columns</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>java.lang.NoClassDefFoundError: javax/serv...</td>\n",
       "      <td>&lt;p&gt;I followed the guide in &lt;a href=\"http://sta...</td>\n",
       "      <td>jsp jstl</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>java.sql.SQLException:[Microsoft][ODBC Dri...</td>\n",
       "      <td>&lt;p&gt;I use the following code&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;...</td>\n",
       "      <td>java jdbc</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0       Implementing Boundary Value Analysis of S...   \n",
       "1           Dynamic Datagrid Binding in Silverlight?   \n",
       "2           Dynamic Datagrid Binding in Silverlight?   \n",
       "3      java.lang.NoClassDefFoundError: javax/serv...   \n",
       "4      java.sql.SQLException:[Microsoft][ODBC Dri...   \n",
       "\n",
       "                                                Body  \\\n",
       "0  <pre><code>#include&lt;iostream&gt;\\n#include&...   \n",
       "1  <p>I should do binding for datagrid dynamicall...   \n",
       "2  <p>I should do binding for datagrid dynamicall...   \n",
       "3  <p>I followed the guide in <a href=\"http://sta...   \n",
       "4  <p>I use the following code</p>\\n\\n<pre><code>...   \n",
       "\n",
       "                                  Tags  cnt_dup  tag_count  \n",
       "0                                c++ c        1          2  \n",
       "1          c# silverlight data-binding        1          3  \n",
       "2  c# silverlight data-binding columns        1          4  \n",
       "3                             jsp jstl        1          2  \n",
       "4                            java jdbc        2          2  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_dup.tag_count.value_counts()\n",
    "df_no_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('train_no_dup.db'):\n",
    "    disk_dup = create_engine(\"sqlite:///train_no_dup.db\")\n",
    "    no_dup = pd.DataFrame(df_no_dup, columns=['Title', 'Body', 'Tags'])\n",
    "\n",
    "    no_dup.to_sql('no_dup_train',disk_dup)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('train_no_dup.db'):\n",
    "    start = datetime.now()\n",
    "    con = sqlite3.connect('train_no_dup.db')\n",
    "    tag_data = pd.read_sql_query(\"\"\"SELECT Tags FROM no_dup_train\"\"\", con)\n",
    "   \n",
    "    con.close()\n",
    "    tag_data.drop(tag_data.index[0], inplace=True)\n",
    "    \n",
    "    tag_data.head()\n",
    "    print(\"Time taken to run this cell :\", datetime.now() - start)\n",
    "else:\n",
    "    print(\"Please download the train.db file from drive or run the above cells to genarate train.db file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\n",
    "tag_dtm = vectorizer.fit_transform(tag_data['Tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points :\", tag_dtm.shape[0])\n",
    "print(\"Number of unique tags :\", tag_dtm.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Some of the tags we have :\", tags[:10])\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = tag_dtm.sum(axis=0).A1\n",
    "result = dict(zip(tags, freqs))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('tag_counts_dict_dtm.csv'):\n",
    "    with open('tag_counts_dict_dtm.csv', 'w') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        for key, value in result.items():\n",
    "            writer.writerow([key, value])\n",
    "tag_df = pd.read_csv(\"tag_counts_dict_dtm.csv\", names=['Tags', 'Counts'])\n",
    "tag_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_df_sorted = tag_df.sort_values(['Counts'], ascending = False)\n",
    "tag_counts = tag_df_sorted['Counts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tag_counts)\n",
    "plt.title(\"distribution of number of times tag appered questions\")\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:10000])\n",
    "plt.title('first 10k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:10000:25]), tag_counts[0:10000:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(tag_counts[0:1000])\n",
    "plt.title('first 1k tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:1000:5]), tag_counts[0:1000:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:500])\n",
    "plt.title('first 500 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.show()\n",
    "print(len(tag_counts[0:500:5]), tag_counts[0:500:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tag_counts[0:100], c='b')\n",
    "plt.scatter(x=list(range(0,100,5)), y=tag_counts[0:100:5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=list(range(0,100,25)), y=tag_counts[0:100:25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "\n",
    "for x,y in zip(list(range(0,100,25)), tag_counts[0:100:25]):\n",
    "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500))\n",
    "\n",
    "plt.title('first 100 tags: Distribution of number of times tag appeared questions')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Tag number\")\n",
    "plt.ylabel(\"Number of times tag appeared\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(len(tag_counts[0:100:5]), tag_counts[0:100:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lst_tags_gt_10k = tag_df[tag_df.Counts>10000].Tags\n",
    "print('{} tags are used more than 10000 times'.format(len(lst_tags_gt_10k)))\n",
    "lst_tags_gt_100k = tag_df[tag_df.Counts>100000].Tags\n",
    "print('{}Tags are used more than 100000 times'.format(len(lst_tags_gt_100k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_quest_count = tag_dtm.sum(axis=1).tolist()\n",
    "tag_quest_count=[int(j) for i in tag_quest_count for j in i]\n",
    "print ('We have total {} datapoints.'.format(len(tag_quest_count)))\n",
    "\n",
    "print(tag_quest_count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Maximum number of tags per question: %d\"%max(tag_quest_count))\n",
    "print( \"Minimum number of tags per question: %d\"%min(tag_quest_count))\n",
    "print( \"Avg. number of tags per question: %f\"% ((sum(tag_quest_count)*1.0)/len(tag_quest_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(tag_quest_count, palette='gist_rainbow')\n",
    "plt.title(\"Number of tags in the questions \")\n",
    "plt.xlabel(\"Number of Tags\")\n",
    "plt.ylabel(\"Number of questions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "# Lets first convert the 'result' dictionary to 'list of tuples'\n",
    "tup = dict(result.items())\n",
    "#Initializing WordCloud using frequencies of tags.\n",
    "wordcloud = WordCloud(    background_color='black',\n",
    "                          width=1600,\n",
    "                          height=800,\n",
    "                    ).generate_from_frequencies(tup)\n",
    "\n",
    "fig = plt.figure(figsize=(30,20))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "fig.savefig(\"tag.png\")\n",
    "plt.show()\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i=np.arange(30)\n",
    "tag_df_sorted.head(30).plot(kind='bar')\n",
    "plt.title('Frequency of top 20 tags')\n",
    "plt.xticks(i, tag_df_sorted['Tags'])\n",
    "plt.xlabel('Tags')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def striphtml(data):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(data))\n",
    "    return cleantext\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#http://www.sqlitetutorial.net/sqlite-python/create-tables/\n",
    "def create_connection(db_file):\n",
    "    \"\"\" create a database connection to the SQLite database\n",
    "        specified by db_file\n",
    "    :param db_file: database file\n",
    "    :return: Connection object or None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None\n",
    "\n",
    "def create_table(conn, create_table_sql):\n",
    "    \"\"\" create a table from the create_table_sql statement\n",
    "    :param conn: Connection object\n",
    "    :param create_table_sql: a CREATE TABLE statement\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = conn.cursor()\n",
    "        c.execute(create_table_sql)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "        \n",
    "def checkTableExists(dbcon):\n",
    "    cursr = dbcon.cursor()\n",
    "    str = \"select name from sqlite_master where type='table'\"\n",
    "    table_names = cursr.execute(str)\n",
    "    print(\"Tables in the databse:\")\n",
    "    tables =table_names.fetchall() \n",
    "    print(tables[0][0])\n",
    "    return(len(tables))\n",
    "\n",
    "def create_database_table(database, query):\n",
    "    conn = create_connection(database)\n",
    "    if conn is not None:\n",
    "        create_table(conn, query)\n",
    "        checkTableExists(conn)\n",
    "    else:\n",
    "        print(\"Error! cannot create the database connection.\")\n",
    "    conn.close()\n",
    "\n",
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Processed.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.sqlitetutorial.net/sqlite-delete/\n",
    "# https://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\n",
    "start = datetime.now()\n",
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500000;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "\n",
    "    is_code = 0\n",
    "\n",
    "    title, question, tags = row[0], row[1], row[2]\n",
    "\n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "\n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "\n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "\n",
    "    title=title.encode('utf-8')\n",
    "\n",
    "    question=str(title)+\" \"+str(question)\n",
    "    question=re.sub(r'[^A-Za-z]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "\n",
    "    #Removing all single letter and and stopwords from question except for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "\n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_db = 'Processed.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_choose(n):\n",
    "    t = multilabel_y.sum(axis=0).tolist()[0]\n",
    "    sorted_tags_i = sorted(range(len(t)), key=lambda i: t[i], reverse=True)\n",
    "    multilabel_yn=multilabel_y[:,sorted_tags_i[:n]]\n",
    "    return multilabel_yn\n",
    "\n",
    "def questions_explained_fn(n):\n",
    "    multilabel_yn = tags_to_choose(n)\n",
    "    x = multilabel_yn.sum(axis=1)\n",
    "    return (np.count_nonzero(x==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 50(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_yx = tags_to_choose(5500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(5500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of tags in sample :\", multilabel_y.shape[1])\n",
    "print(\"number of tags taken :\", multilabel_yx.shape[1],\"(\",(multilabel_yx.shape[1]/multilabel_y.shape[1])*100,\"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_size=preprocessed_data.shape[0]\n",
    "train_size=int(0.80*total_size)\n",
    "\n",
    "x_train=preprocessed_data.head(train_size)\n",
    "x_test=preprocessed_data.tail(total_size - train_size)\n",
    "\n",
    "y_train = multilabel_yx[0:train_size,:]\n",
    "y_test = multilabel_yx[train_size:total_size,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_create_table = \"\"\"CREATE TABLE IF NOT EXISTS QuestionsProcessed (question text NOT NULL, code text, tags text, words_pre integer, words_post integer, is_code integer);\"\"\"\n",
    "create_database_table(\"Titlemoreweight.db\", sql_create_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_db = 'train_no_dup.db'\n",
    "write_db = 'Titlemoreweight.db'\n",
    "train_datasize = 400000\n",
    "if os.path.isfile(read_db):\n",
    "    conn_r = create_connection(read_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        # for selecting first 0.5M rows\n",
    "        reader.execute(\"SELECT Title, Body, Tags From no_dup_train LIMIT 500001;\")\n",
    "        # for selecting random points\n",
    "        #reader.execute(\"SELECT Title, Body, Tags From no_dup_train ORDER BY RANDOM() LIMIT 500001;\")\n",
    "\n",
    "if os.path.isfile(write_db):\n",
    "    conn_w = create_connection(write_db)\n",
    "    if conn_w is not None:\n",
    "        tables = checkTableExists(conn_w)\n",
    "        writer =conn_w.cursor()\n",
    "        if tables != 0:\n",
    "            writer.execute(\"DELETE FROM QuestionsProcessed WHERE 1\")\n",
    "            print(\"Cleared All the rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "preprocessed_data_list=[]\n",
    "reader.fetchone()\n",
    "questions_with_code=0\n",
    "len_pre=0\n",
    "len_post=0\n",
    "questions_proccesed = 0\n",
    "for row in reader:\n",
    "    \n",
    "    is_code = 0\n",
    "    \n",
    "    title, question, tags = row[0], row[1], str(row[2])\n",
    "    \n",
    "    if '<code>' in question:\n",
    "        questions_with_code+=1\n",
    "        is_code = 1\n",
    "    x = len(question)+len(title)\n",
    "    len_pre+=x\n",
    "    \n",
    "    code = str(re.findall(r'<code>(.*?)</code>', question, flags=re.DOTALL))\n",
    "    \n",
    "    question=re.sub('<code>(.*?)</code>', '', question, flags=re.MULTILINE|re.DOTALL)\n",
    "    question=striphtml(question.encode('utf-8'))\n",
    "    \n",
    "    title=title.encode('utf-8')\n",
    "    \n",
    "    # adding title three time to the data to increase its weight\n",
    "    # add tags string to the training data\n",
    "    \n",
    "    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "    \n",
    "#     if questions_proccesed<=train_datasize:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question+\" \"+str(tags)\n",
    "#     else:\n",
    "#         question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+question\n",
    "\n",
    "    question=re.sub(r'[^A-Za-z0-9#+.\\-]+',' ',question)\n",
    "    words=word_tokenize(str(question.lower()))\n",
    "    \n",
    "    #Removing all single letter and and stopwords from question exceptt for the letter 'c'\n",
    "    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n",
    "    \n",
    "    len_post+=len(question)\n",
    "    tup = (question,code,tags,x,len(question),is_code)\n",
    "    questions_proccesed += 1\n",
    "    writer.execute(\"insert into QuestionsProcessed(question,code,tags,words_pre,words_post,is_code) values (?,?,?,?,?,?)\",tup)\n",
    "    if (questions_proccesed%100000==0):\n",
    "        print(\"number of questions completed=\",questions_proccesed)\n",
    "\n",
    "no_dup_avg_len_pre=(len_pre*1.0)/questions_proccesed\n",
    "no_dup_avg_len_post=(len_post*1.0)/questions_proccesed\n",
    "\n",
    "print( \"Avg. length of questions(Title+Body) before processing: %d\"%no_dup_avg_len_pre)\n",
    "print( \"Avg. length of questions(Title+Body) after processing: %d\"%no_dup_avg_len_post)\n",
    "print (\"Percent of questions containing code: %d\"%((questions_with_code*100.0)/questions_proccesed))\n",
    "\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_r.commit()\n",
    "conn_w.commit()\n",
    "conn_r.close()\n",
    "conn_w.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        reader =conn_r.cursor()\n",
    "        reader.execute(\"SELECT question From QuestionsProcessed LIMIT 10\")\n",
    "        print(\"Questions after preprocessed\")\n",
    "        print('='*100)\n",
    "        reader.fetchone()\n",
    "        for row in reader:\n",
    "            print(row)\n",
    "            print('-'*100)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_db = 'Titlemoreweight.db'\n",
    "if os.path.isfile(write_db):\n",
    "    conn_r = create_connection(write_db)\n",
    "    if conn_r is not None:\n",
    "        preprocessed_data = pd.read_sql_query(\"\"\"SELECT question, Tags FROM QuestionsProcessed\"\"\", conn_r)\n",
    "conn_r.commit()\n",
    "conn_r.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of data points in sample :\", preprocessed_data.shape[0])\n",
    "print(\"number of dimensions :\", preprocessed_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\n",
    "multilabel_y = vectorizer.fit_transform(preprocessed_data['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_explained = []\n",
    "total_tags=multilabel_y.shape[1]\n",
    "total_qs=preprocessed_data.shape[0]\n",
    "for i in range(500, total_tags, 100):\n",
    "    questions_explained.append(np.round(((total_qs-questions_explained_fn(i))/total_qs)*100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(questions_explained)\n",
    "xlabel = list(500+np.array(range(-50,450,50))*50)\n",
    "ax.set_xticklabels(xlabel)\n",
    "plt.xlabel(\"Number of tags\")\n",
    "plt.ylabel(\"Number Questions coverd partially\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# you can choose any number of tags based on your computing power, minimun is 500(it covers 90% of the tags)\n",
    "print(\"with \",5500,\"tags we are covering \",questions_explained[50],\"% of questions\")\n",
    "print(\"with \",500,\"tags we are covering \",questions_explained[0],\"% of questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_yx = tags_to_choose(500)\n",
    "print(\"number of questions that are not covered :\", questions_explained_fn(500),\"out of \", total_qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=preprocessed_data.head(train_datasize)\n",
    "x_test=preprocessed_data.tail(preprocessed_data.shape[0] - 400000)\n",
    "\n",
    "y_train = multilabel_yx[0:train_datasize,:]\n",
    "y_test = multilabel_yx[train_datasize:preprocessed_data.shape[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data :\", y_train.shape)\n",
    "print(\"Number of data points in test data :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "vectorizer = CountVectorizer(min_df=0.00009, max_features=200000, tokenizer=lambda x: x.split(), ngram_range=(1,4))\n",
    "x_train_multilabel = vectorizer.fit_transform(x_train['question'])\n",
    "x_test_multilabel = vectorizer.transform(x_test['question'])\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of train data X:\",x_train_multilabel.shape, \"Y :\",y_train.shape)\n",
    "print(\"Dimensions of test data X:\",x_test_multilabel.shape,\"Y:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "classifier_2 = OneVsRestClassifier(LogisticRegression(penalty='l1'))\n",
    "classifier_2.fit(x_train_multilabel, y_train)\n",
    "predictions_2 = classifier_2.predict(x_test_multilabel)\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions_2))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions_2))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='micro')\n",
    "recall = recall_score(y_test, predictions_2, average='micro')\n",
    "f1 = f1_score(y_test, predictions_2, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions_2, average='macro')\n",
    "recall = recall_score(y_test, predictions_2, average='macro')\n",
    "f1 = f1_score(y_test, predictions_2, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions_2))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "start = datetime.now()\n",
    "\n",
    "param_grid = dict(estimator__C=[0.001,0.01,1,100,1000])\n",
    "\n",
    "gsv = GridSearchCV(OneVsRestClassifier(LogisticRegression()), param_grid=param_grid, verbose=5, n_jobs=-1)\n",
    "gsv.fit(x_train_multilabel, y_train)\n",
    "\n",
    "print('The best value of hyper parameter is ', gsv.best_params_)\n",
    "print('The best score is ', gsv.best_score_)\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ = gsv.best_params_['estimator__C']\n",
    "print(gsv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = OneVsRestClassifier(LogisticRegression(C=c_))\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = OneVsRestClassifier(LogisticRegression(C=0.01, penalty='l1'))\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "classifier = OneVsRestClassifier(SGDClassifier(loss='hinge', alpha=0.00001, penalty='l1'), n_jobs=-1)\n",
    "classifier.fit(x_train_multilabel, y_train)\n",
    "predictions = classifier.predict (x_test_multilabel)\n",
    "\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test, predictions))\n",
    "print(\"Hamming loss \",metrics.hamming_loss(y_test,predictions))\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='micro')\n",
    "recall = recall_score(y_test, predictions, average='micro')\n",
    "f1 = f1_score(y_test, predictions, average='micro')\n",
    " \n",
    "print(\"Micro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    " \n",
    "print(\"Macro-average quality numbers\")\n",
    "print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(precision, recall, f1))\n",
    "\n",
    "print (metrics.classification_report(y_test, predictions))\n",
    "print(\"Time taken to run this cell :\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Classification model\", \"Hyperparameter\", \"Regularization\", \"F1 micro\", \"F1 macro\"]\n",
    "\n",
    "x.add_row([\"Logistic Regression\", 1, \"L1\", 0.4762, 0.3807])\n",
    "x.add_row([\"Logistic Regression\", 0.01, \"L2\", 0.4571, 0.3459])\n",
    "x.add_row([\"Logistic Regression\", 0.01, \"L1\",0.4131, 0.2966])\n",
    "x.add_row([\"Linear SVM\", 0.0001, \"L1\", 0.3601, 0.2693])\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
